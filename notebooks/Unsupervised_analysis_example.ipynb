{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bd014d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: start with fcc image, then see transition and all these features\n",
    "# then add more images, Ti for example, and then add Fe bcc as final example.\n",
    "# maybe precalculate the hidden reps\n",
    "# ***Add visualization feature where can hover over the images and sees them.***\n",
    "# Better to provide pre-calculated values and then some code how to do it for \n",
    "# new images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd00845",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to use UMAP, a manifold learning algorithm, to inspect the internal neural-network representations of AI-STEM that are learned during training. \n",
    "\n",
    "First we import the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2364a376",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install 'git+https://github.com/AndreasLeitherer/ai4stem.git'\n",
    "! pip install tensorflow\n",
    "! pip install opencv-python\n",
    "! pip install umap-learn\n",
    "! pip install bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453e8b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# tensorflow info/warnings switched off\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from ai4stem.utils.utils_data import load_pretrained_model, load_example_image\n",
    "from ai4stem.utils.utils_prediction import predict\n",
    "\n",
    "from ai4stem.utils.utils_fft import calc_fft\n",
    "from ai4stem.utils.utils_prediction import localwindow\n",
    "from ai4stem.utils.utils_nn import decode_preds, predict_with_uncertainty\n",
    "from ai4stem.utils.utils_data import load_class_dicts\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import umap\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 10})\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db4e2f9",
   "metadata": {},
   "source": [
    "Next, we load an example image, here Fe bcc in [100] orientation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b07b5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image\n",
    "image = load_example_image()\n",
    "image_name = 'Fe_bcc'\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977527d8",
   "metadata": {},
   "source": [
    "Now we want to investigate how the AI-STEM's internal neural-network representations represent local fragments of this image, for which we first perform the fragmentation and desciptor-calculation steps (as done in the quickstart [notebook](https://colab.research.google.com/github/AndreasLeitherer/ai4stem/blob/main/notebooks/Application_of_pretrained_model.ipynb)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a9f2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pixel/Angstrom relation\n",
    "pixel_to_angstrom = 0.1245\n",
    "# AI-STEM parameters\n",
    "window_size = 12.\n",
    "stride_size = [36, 36]\n",
    "# convert window [Angstrom] to window [pixels]\n",
    "adapted_window_size = int(window_size * (1. / pixel_to_angstrom))\n",
    "\n",
    "logger.info('Fragmentation.')\n",
    "# calc fft\n",
    "sliced_images, spm_pos, ni, nj = localwindow(image, \n",
    "                                             stride_size=stride_size, \n",
    "                                             pixel_max=adapted_window_size)\n",
    "\n",
    "logger.info('Calculate FFT-HAADF descriptor.')\n",
    "fft_descriptors = []\n",
    "for im in sliced_images:\n",
    "    fft_desc = calc_fft(im, sigma=None, thresholding=True)\n",
    "    fft_descriptors.append(fft_desc)\n",
    "    \n",
    "# reshape such that matches model input shape\n",
    "data = np.array([np.stack([_]) for _ in fft_descriptors])\n",
    "data = np.moveaxis(data, 1, -1)\n",
    "logger.info('Finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c37bdb",
   "metadata": {},
   "source": [
    "Now we want to extract the neural-network representations, where we inspect the last layer before classification is performed (this layer's name is 'Dense_1'). We first load the pretrained model and then truncate it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bad0827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model\n",
    "model = load_pretrained_model()\n",
    "\n",
    "# Define model, where remove last classification layer\n",
    "inputs = model.input\n",
    "# select layer before last classification layer\n",
    "# as new final layer:\n",
    "outpout_layer_name = 'Dense_1' \n",
    "outputs = model.get_layer(outpout_layer_name).output\n",
    "intermediate_layer_model = Model(inputs=inputs,\n",
    "                                 outputs=outputs)\n",
    "intermediate_layer_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefd3eb1",
   "metadata": {},
   "source": [
    "Using this truncated model, we have access to the hidden representations (expected exeuction time ~30 seconds):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab63dcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute representations\n",
    "nn_representations = decode_preds(data, intermediate_layer_model, n_iter=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca0bf44",
   "metadata": {},
   "source": [
    "Now we can apply the Uniform Manifold Approximation and Projection (UMAP) algorithm to visualize the hidden space. We explore some of the most important parameters in UMAP and visualize them in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72aceb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply UMAP\n",
    "\n",
    "# most important parameter:\n",
    "# number of neighbors employed\n",
    "# for calculating low-dimensional (here, 2D)\n",
    "# embedding, we try a range of values, where\n",
    "# we use 1%, 5% and 10% of the data set size as # neighbors\n",
    "n_neighbors_list = nn_representations.shape[0] * np.array([0.02, 0.05, 0.1]) \n",
    "n_neighbors_list = n_neighbors_list.astype(int)\n",
    "# Choose minimum distance (0<min_dist<1.0) \n",
    "# which controls the spread of the points\n",
    "# in the low-dimensional embedding (only for improving visualization)\n",
    "min_dist_list = [0.1, 0.5, 0.9]\n",
    "# choose Euclidean metric\n",
    "# for measuring distance between data points\n",
    "metric = 'euclidean'\n",
    "# Choose 2 as embedding dimension\n",
    "n_components = 2\n",
    "# plotting parameters\n",
    "s = 2.5\n",
    "edgecolors = 'face'\n",
    "\n",
    "data_for_fitting = nn_representations\n",
    "\n",
    "results = dict()\n",
    "\n",
    "for i, n_neighbors in enumerate(n_neighbors_list):\n",
    "    for j, min_dist in enumerate(min_dist_list):\n",
    "        logger.info('Calculate UMAP embedding for # neighbors = {}, min. distance = {}'.format(n_neighbors, min_dist))\n",
    "        mapper = umap.UMAP(n_neighbors=n_neighbors, \n",
    "                           min_dist=min_dist,\n",
    "                           metric=metric,\n",
    "                           n_components=n_components).fit(data_for_fitting)\n",
    "        embedding = mapper.transform(data_for_fitting)\n",
    "        \n",
    "        results[(n_neighbors, min_dist)] = embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2e9c6d",
   "metadata": {},
   "source": [
    "Now let us visualize the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1088ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(len(min_dist_list), len(n_neighbors_list), figsize=(15, 15))\n",
    "for i, n_neighbors in enumerate(n_neighbors_list):\n",
    "    for j, min_dist in enumerate(min_dist_list):\n",
    "        embedding = results[(n_neighbors, min_dist)]\n",
    "        im = axs[i, j].scatter(embedding[:, 0], embedding[:, 1],\n",
    "                               s=s)\n",
    "        axs[i, j].set_aspect('equal')\n",
    "        axs[i, j].axis('off')\n",
    "        axs[i, j].set_title('# Neighbors = {},\\n min_dist = {}'.format(n_neighbors, min_dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46658220",
   "metadata": {},
   "source": [
    "We can see that for small # neighbors, no patterns can be observed, while for larger values, two main clusters emerge. The minimium distance on the other hand controls the spread of the points in both clusters.\n",
    "\n",
    "Now we would like to know what physical meaning these clusters have. For that, we calculate the AI-STEM assignments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2da4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction, uncertainty = predict_with_uncertainty(data, model, \n",
    "                                                   model_type='classification', \n",
    "                                                   n_iter=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d14cd8",
   "metadata": {},
   "source": [
    "Let us first check which symmetry is assigned to the main clusters - by chooinsg the most likely label as the color scale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a4aa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai4stem.utils.utils_data import load_class_dicts\n",
    "\n",
    "numerical_to_text_labels, text_to_numerical_labels = load_class_dicts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4535206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "color_scale = prediction.argmax(axis=-1)\n",
    "# relation between int class labels and text labels:\n",
    "numerical_to_text_labels, text_to_numerical_labels = load_class_dicts()\n",
    "\n",
    "fig, axs = plt.subplots(len(min_dist_list), len(n_neighbors_list), figsize=(15, 15))\n",
    "for i, n_neighbors in enumerate(n_neighbors_list):\n",
    "    for j, min_dist in enumerate(min_dist_list):\n",
    "        embedding = results[(n_neighbors, min_dist)]\n",
    "        #im = axs[i, j].scatter(embedding[:, 0], embedding[:, 1],\n",
    "        #                       s=s, c=color_scale, cmap='tab10')\n",
    "        df = pd.DataFrame({'e1': embedding[:, 0], 'e2': embedding[:, 1], \n",
    "                           'target': [numerical_to_text_labels[str(_)] for _ in color_scale]})\n",
    "        im = sns.scatterplot(x=\"e1\", y=\"e2\", hue=\"target\", \n",
    "                             data=df, ax=axs[i, j], palette='tab10', s=s)\n",
    "        im.set(xticks=[])\n",
    "        im.set(yticks=[])\n",
    "        im.set(xlabel=None)\n",
    "        im.set(ylabel=None)\n",
    "        \n",
    "        axs[i, j].set_aspect('equal')\n",
    "        axs[i, j].legend(loc='lower right')\n",
    "        # sns.move_legend(axs[i, j], \"lower left\", bbox_to_anchor=(1, 0.5))\n",
    "        axs[i, j].set_title('# Neighbors = {},\\n min_dist = {}'.format(n_neighbors, min_dist))\n",
    "        #fig.colorbar(im, ax=axs[i, j])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4469fd3e",
   "metadata": {},
   "source": [
    "We can see that the two clusters are assigned the same label with some exceptions as one can see in the sub-clusters that appear. Based on the assignmnets, we can already infer that these correspond to the interface region. To further confirm that, we also use the mutual information as color scale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafaec25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "color_scale = uncertainty['mutual_information']\n",
    "\n",
    "fig, axs = plt.subplots(len(min_dist_list), len(n_neighbors_list), figsize=(15, 15))\n",
    "for i, n_neighbors in enumerate(n_neighbors_list):\n",
    "    for j, min_dist in enumerate(min_dist_list):\n",
    "        embedding = results[(n_neighbors, min_dist)]\n",
    "        im = axs[i, j].scatter(embedding[:, 0], embedding[:, 1],\n",
    "                               s=s, c=color_scale, cmap='hot')\n",
    "\n",
    "        axs[i, j].set_xticks([])\n",
    "        axs[i, j].set_yticks([])\n",
    "        \n",
    "        axs[i, j].set_aspect('equal')\n",
    "        axs[i, j].set_title('# Neighbors = {},\\n min_dist = {}'.format(n_neighbors, min_dist))\n",
    "        fig.colorbar(im, ax=axs[i, j])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847dba58",
   "metadata": {},
   "source": [
    "We have thus visualized how the network separaters bulk and interface regions.\n",
    "\n",
    "To add one more tool for visualization, please find the following hover plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f086d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import base64\n",
    "! pip install bokeh\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import HoverTool, ColumnDataSource, CategoricalColorMapper\n",
    "from bokeh.palettes import Spectral10\n",
    "output_notebook()\n",
    "\n",
    "def embeddable_image(data):\n",
    "    data = cv2.normalize(data, None,\n",
    "                       alpha=0, beta=1,\n",
    "                       norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "    \n",
    "    #img_data = 255 - 15 * data#.astype(np.uint8)\n",
    "    #image = Image.fromarray(img_data, mode='L')\n",
    "    # image = image.convert(\"L\")\n",
    "    data = (255 * data).astype(np.uint8)\n",
    "    image = Image.fromarray(data)\n",
    "    image = image.convert(\"L\")\n",
    "    buffer = BytesIO()\n",
    "    image.save(buffer, format='jpeg')\n",
    "    for_encoding = buffer.getvalue()\n",
    "    return 'data:image/png;base64,' + base64.b64encode(for_encoding).decode()\n",
    "\n",
    "\n",
    "digits = {}\n",
    "digits['target'] = prediction.argmax(axis=-1)\n",
    "digits['target_names'] = [numerical_to_text_labels[str(_)] for _ in prediction.argmax(axis=-1)]\n",
    "digits['images'] = sliced_images\n",
    "\n",
    "#####################################\n",
    "digits_df = pd.DataFrame(embedding, columns=('x', 'y'))\n",
    "digits_df['digit'] = [str(x) for x in digits['target']]\n",
    "digits_df['image'] = list(map(embeddable_image, digits['images']))\n",
    "\n",
    "datasource = ColumnDataSource(digits_df)\n",
    "color_mapping = CategoricalColorMapper(factors=[x for x in np.unique(digits['target_names'])],\n",
    "                                       palette=Spectral10)\n",
    "\n",
    "plot_figure = figure(\n",
    "    title='UMAP projection Fe bcc [100] HAADF STEM image',\n",
    "    plot_width=600,\n",
    "    plot_height=600,\n",
    "    tools=('pan, wheel_zoom, reset')\n",
    ")\n",
    "\n",
    "plot_figure.add_tools(HoverTool(tooltips=\"\"\"\n",
    "<div>\n",
    "    <div>\n",
    "        <img src='@image' style='float: left; margin: 5px 5px 5px 5px'/>\n",
    "    </div>\n",
    "    <div>\n",
    "        <span style='font-size: 16px; color: #224499'>Label:</span>\n",
    "        <span style='font-size: 18px'>@digit</span>\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\"))\n",
    "\n",
    "plot_figure.circle(\n",
    "    'x',\n",
    "    'y',\n",
    "    source=datasource,\n",
    "    color=dict(field='digit', transform=color_mapping),\n",
    "    line_alpha=0.6,\n",
    "    fill_alpha=0.6,\n",
    "    size=10\n",
    ")\n",
    "show(plot_figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dcb2b4",
   "metadata": {},
   "source": [
    "The question is now: what happens if we consider different interfaces? Will they be assigned the same cluster? Does our choice of training set and optimization routine make it impossible to distinguish different interface types?\n",
    "\n",
    "The answer is no - we can distinguish different interfaces, and we will demonstrate that in the following - by considering three experimental images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47b69c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_link_fcc = 'https://www.dropbox.com/s/flfy5qe1qxv47t6/Cu_fcc_111.npy?dl=0'\n",
    "download_link_bcc = 'https://www.dropbox.com/s/ukab367rktmddse/Fe_bcc_100.npy?dl=0'\n",
    "download_link_hcp = 'https://www.dropbox.com/s/q4rvqcy87u3ath9/Ti_hcp_0001.npy?dl=0'\n",
    "\n",
    "!wget -q $download_link_fcc -O 'Cu_fcc_100.npy'\n",
    "!wget -q $download_link_bcc -O 'Fe_bcc_100.npy'\n",
    "!wget -q $download_link_hcp -O 'Ti_hcp_0001.npy'\n",
    "\n",
    "images = [np.load('Cu_fcc_100.npy'),\n",
    "          np.load('Fe_bcc_100.npy'),\n",
    "          np.load('Ti_hcp_0001.npy')]\n",
    "\n",
    "image_names = ['Cu_fcc_100',\n",
    "               'Fe_bcc_100',\n",
    "               'Ti_hcp_0001']\n",
    "\n",
    "pixel_to_angstrom= [0.08805239,\n",
    "                    0.12452489,\n",
    "                    0.12452489]\n",
    "\n",
    "adapted_window_sizes = [int(window_size * (1. / ratio)) for ratio in  pixel_to_angstrom]\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(20,20))\n",
    "axs[0].imshow(images[0], cmap='gray')\n",
    "axs[0].set_title('Cu fcc [100]')\n",
    "axs[1].imshow(images[1], cmap='gray')\n",
    "axs[1].set_title('Fe bcc [100]')\n",
    "axs[2].imshow(images[2], cmap='gray')\n",
    "axs[2].set_title('Ti hcp [0001]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30eeb59d",
   "metadata": {},
   "source": [
    "Next, we load precalculated neural-network representations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1f03d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_nn_rep_fcc = 'https://www.dropbox.com/s/e4ny6a1ird1v7u8/nn_representations_Cu_fcc_100.npy?dl=0'\n",
    "url_nn_rep_bcc = 'https://www.dropbox.com/s/wbpjgiwyd0iozgm/nn_representations_Fe_bcc_100.npy?dl=0'\n",
    "url_nn_rep_hcp = 'https://www.dropbox.com/s/l59chdveknm4mq5/nn_representations_Ti_hcp_0001.npy?dl=0'\n",
    "    \n",
    "!wget -q $url_nn_rep_fcc -O 'nn_rep_Cu_fcc_100.npy'\n",
    "!wget -q $url_nn_rep_bcc -O 'nn_rep_Fe_bcc_100.npy'\n",
    "!wget -q $url_nn_rep_hcp -O 'nn_rep_Ti_hcp_0001.npy'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bbd598",
   "metadata": {},
   "source": [
    "To calculate these representations, we employed a 12x12 pixels stride for Cu and Ti, while reducing the stride to 6x6 pixels for Fe. A window size of 12 Angstrom is selected (corresponding to 96 pixels for Fe, Ti and 136 for Cu).\n",
    "\n",
    "Next, we concatenate these representations in order to calculate the embedding into 2D via UMAP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac51be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_representations = [np.load('nn_rep_{}.npy'.format(_)) for _ in image_names]\n",
    "nn_representations_combined = np.concatenate(nn_representations, axis=0)\n",
    "print(nn_representations_combined.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7b1cad",
   "metadata": {},
   "source": [
    "To be able to conduct a similar analysis as done for the Fe bcc [100] image before, we also load precalculated  assignments and uncertainty estimates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87f7f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_assignments_fcc = 'https://www.dropbox.com/s/e0hav8kkufat57w/assignments_Cu_fcc_100.npy?dl=0'\n",
    "url_assignments_bcc = 'https://www.dropbox.com/s/ovtadac1whxkm2g/assignments_Fe_bcc_100.npy?dl=0'\n",
    "url_assignments_hcp = 'https://www.dropbox.com/s/ksrayk5dopizdqa/assignments_Ti_hcp_0001.npy?dl=0'\n",
    "\n",
    "!wget -q $url_assignments_fcc -O 'assignments_Cu_fcc_100.npy'\n",
    "!wget -q $url_assignments_bcc -O 'assignments_Fe_bcc_100.npy'\n",
    "!wget -q $url_assignments_hcp -O 'assignments_Ti_hcp_0001.npy'\n",
    "\n",
    "assignments = [np.load('assignments_{}.npy'.format(_)) for _ in image_names]\n",
    "\n",
    "url_uncertainty_fcc = 'https://www.dropbox.com/s/hy1hrr4rq22cqgu/uncertainty_Cu_fcc_100.npy?dl=0'\n",
    "url_uncertainty_bcc = 'https://www.dropbox.com/s/y9g5r1u0k3h7vvs/uncertainty_Fe_bcc_100.npy?dl=0'\n",
    "url_uncertainty_hcp = 'https://www.dropbox.com/s/e1mx9rjeyadg4m9/uncertainty_Ti_hcp_0001.npy?dl=0'\n",
    "\n",
    "!wget -q $url_uncertainty_fcc -O 'uncertainty_Cu_fcc_100.npy'\n",
    "!wget -q $url_uncertainty_bcc -O 'uncertainty_Fe_bcc_100.npy'\n",
    "!wget -q $url_uncertainty_hcp -O 'uncertainty_Ti_hcp_0001.npy'\n",
    "\n",
    "uncertainty = [np.load('uncertainty_{}.npy'.format(_)) for _ in image_names]\n",
    "\n",
    "assignments_combined = np.concatenate(assignments, axis=0)\n",
    "uncertainty_combined = np.concatenate(uncertainty, axis=0)\n",
    "print(assignments_combined.shape, uncertainty_combined.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f201ad",
   "metadata": {},
   "source": [
    "Now we calculated the UMAP embedding, where we choose a specific number of neighbors and minimum distance value (other values may be easily tested, see above for the code to test different settings of, for instance, number of neighbors and minimum distance):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b297a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = 500\n",
    "min_dist = 0.9\n",
    "metric = 'euclidean'\n",
    "n_components = 2\n",
    "s = 1\n",
    "\n",
    "data_for_fitting = nn_representations_combined\n",
    "\n",
    "mapper = umap.UMAP(n_neighbors=n_neighbors, \n",
    "                   min_dist=min_dist,\n",
    "                   metric=metric,\n",
    "                   n_components=n_components).fit(data_for_fitting)\n",
    "embedding = mapper.transform(data_for_fitting)\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(10, 10))\n",
    "axs.scatter(embedding[:, 0], embedding[:, 1], s=s)\n",
    "axs.set_xticks([])\n",
    "axs.set_yticks([])\n",
    "\n",
    "axs.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629fd06a",
   "metadata": {},
   "source": [
    "Employing assignments and mutual information as color scales, respectively, supports the above claim of AI-STEM being able to separate not only different bulk symmetries or bulk from interface regions - but also different interface types, despite never being explicitly instructed to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc664860",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(20, 20), gridspec_kw={'width_ratios': [0.91, 1]})\n",
    "\n",
    "df = pd.DataFrame({'e1': embedding[:, 0], 'e2': embedding[:, 1], \n",
    "                   'target': assignments_combined})\n",
    "im = sns.scatterplot(x=\"e1\", y=\"e2\", hue=\"target\", \n",
    "                     data=df, ax=axs[0], palette='tab10', s=s)\n",
    "im.set(xticks=[])\n",
    "im.set(yticks=[])\n",
    "im.set(xlabel=None)\n",
    "im.set(ylabel=None)\n",
    "axs[0].set_aspect('equal')\n",
    "axs[0].legend(loc='lower right')\n",
    "axs[0].set_title('Color scale: most likely class')\n",
    "\n",
    "\n",
    "im = axs[1].scatter(embedding[:, 0], embedding[:, 1],\n",
    "                    s=s, c=uncertainty_combined, cmap='hot')\n",
    "axs[1].set_xticks([])\n",
    "axs[1].set_yticks([])\n",
    "axs[1].set_aspect('equal')\n",
    "axs[1].set_title('Color scale: Bayesian uncertainty (mutual information)')\n",
    "fig.colorbar(im, ax=axs[1], fraction=0.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1eca05",
   "metadata": {},
   "source": [
    "# Apply UMAP and visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5308e135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dictionaries for visalizing\n",
    "layer_activations = {'nn_rep': nn_representations}\n",
    "targets = {'nn_rep': {'argmax': prediction.argmax(axis=-1), 'mut_info': uncertainty['mutual_information']}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08197dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply UMAP\n",
    "\n",
    "# most important parameter:\n",
    "# number of neighbors employed\n",
    "# for calculating low-dimensional (here, 2D)\n",
    "# embedding\n",
    "n_neighbors_list = [5, 50, 200]\n",
    "# choose Euclidean metric\n",
    "# for measuring distance between data points\n",
    "metric = 'euclidean'\n",
    "# Choose 2 as embedding dimension\n",
    "n_components = 2\n",
    "# plotting parameters\n",
    "s = 2.5\n",
    "edgecolors = 'face'\n",
    "\n",
    "for n_neighbors in n_neighbors_list:\n",
    "    logger.info('Apply UMAP for number of neighbors = {}'.format(n_neighbors))\n",
    "\n",
    "    for key in layer_activations:\n",
    "        \n",
    "        data_for_fitting = layer_activations[key]\n",
    "\n",
    "        mapper1 = umap.UMAP(n_neighbors=n_neighbors, \n",
    "                            metric=metric, \n",
    "                            n_components=n_components).fit(data_for_fitting)\n",
    "        embedding = mapper1.transform(data_for_fitting)\n",
    "\n",
    "        for target in targets[key]:\n",
    "            cmap = None\n",
    "            nber_unique_colors = np.unique(targets[key][target]).size\n",
    "            if target == 'mut_info':\n",
    "                cmap = 'hot'\n",
    "            else:\n",
    "                cmap = 'tab10'\n",
    "            fig, axs = plt.subplots(facecolor='white', figsize=(10, 10))\n",
    "            df = pd.DataFrame({'e1': embedding[:, 0], 'e2': embedding[:, 1], 'target': targets[key][target]})\n",
    "            \n",
    "            if target == 'argmax_pred':\n",
    "                df['target'] = [text_to_numerical_label[_] for _ in df['target'].values]\n",
    "            \n",
    "            im = axs.scatter(df['e1'].values, df['e2'].values, c=df['target'], cmap=cmap, s=s)\n",
    "            axs.set_aspect('equal')\n",
    "            fig.colorbar(im, ax=axs)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # uncomment if want to save\n",
    "            #plt.savefig('{}_{}_nn_{}_embedding.png'.format(key, target, n_neighbors), dpi=200)\n",
    "            #plt.close()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d50f7d",
   "metadata": {},
   "source": [
    "# Repeat analysis for several images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6832c4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_link_fcc = 'https://www.dropbox.com/s/flfy5qe1qxv47t6/Cu_fcc_111.npy?dl=0'\n",
    "download_link_bcc = 'https://www.dropbox.com/s/ukab367rktmddse/Fe_bcc_100.npy?dl=0'\n",
    "download_link_hcp = 'https://www.dropbox.com/s/q4rvqcy87u3ath9/Ti_hcp_0001.npy?dl=0'\n",
    "\n",
    "!wget -q $download_link_fcc -O 'Cu_fcc_100.npy'\n",
    "!wget -q $download_link_bcc -O 'Fe_bcc_100.npy'\n",
    "!wget -q $download_link_hcp -O 'Ti_hcp_0001.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caa600f",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [np.load('Cu_fcc_100.npy'),\n",
    "          np.load('Fe_bcc_100.npy'),\n",
    "          np.load('Ti_hcp_0001.npy')]\n",
    "\n",
    "image_names = ['Cu_fcc_100',\n",
    "               'Fe_bcc_100',\n",
    "               'Ti_hcp_0001']\n",
    "\n",
    "window_size = 12. # units: Angstrom\n",
    "\n",
    "pixel_to_angstrom= [0.08805239,\n",
    "                    0.12452489,\n",
    "                    0.12452489]\n",
    "\n",
    "strides = [[12, 12], [6, 6], [12, 12]]\n",
    "strides = [[36, 36], [36, 36], [36, 36]]\n",
    "\n",
    "adapted_window_sizes = [int(window_size * (1. / ratio)) for ratio in  pixel_to_angstrom]\n",
    "print(adapted_window_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9d1a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(20,20))\n",
    "axs[0].imshow(images[0], cmap='gray')\n",
    "axs[0].set_title('Cu fcc [100]')\n",
    "axs[1].imshow(images[1], cmap='gray')\n",
    "axs[1].set_title('Fe bcc [100]')\n",
    "axs[2].imshow(images[2], cmap='gray')\n",
    "axs[2].set_title('Ti hcp [0001]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ccda14",
   "metadata": {},
   "outputs": [],
   "source": [
    "fft_descriptors = []\n",
    "for idx, input_image in enumerate(images):\n",
    "    \n",
    "    stride_size = strides[idx]\n",
    "    adapted_window_size = adapted_window_sizes[idx]\n",
    "    image_name = image_names[idx]\n",
    "    \n",
    "    logger.info('Extract local fragments for image {}.'.format(image_name))\n",
    "    # calc fft\n",
    "    sliced_images, spm_pos, ni, nj = localwindow(input_image, \n",
    "                                                 stride_size=stride_size, \n",
    "                                                 pixel_max=adapted_window_size)\n",
    "\n",
    "    logger.info('Calculate FFT-HAADF descriptor for image {}.'.format(image_name))\n",
    "    \n",
    "    for im in sliced_images:\n",
    "        fft_desc = calc_fft(im, sigma=None, thresholding=True)\n",
    "        fft_descriptors.append(fft_desc)\n",
    "\n",
    "# reshape such that matches model input shape\n",
    "data = np.array([np.stack([_]) for _ in fft_descriptors])\n",
    "data = np.moveaxis(data, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d62dcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model, where remove last classification layer\n",
    "\n",
    "inputs = model.input\n",
    "# select layer before last classification layer\n",
    "# as new final layer:\n",
    "outpout_layer_name = 'Dense_1' \n",
    "outputs = model.get_layer(outpout_layer_name).output\n",
    "intermediate_layer_model = Model(inputs=inputs,\n",
    "                                 outputs=outputs)\n",
    "intermediate_layer_model.summary()\n",
    "\n",
    "\n",
    "# Compute representations\n",
    "nn_representations = decode_preds(data, intermediate_layer_model, n_iter=10)\n",
    "prediction, uncertainty = predict_with_uncertainty(data, model, \n",
    "                                                   model_type='classification', \n",
    "                                                   n_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b6fc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dictionaries for visalizing\n",
    "layer_activations = {'nn_rep': nn_representations}\n",
    "targets = {'nn_rep': {'argmax': prediction.argmax(axis=-1), 'mut_info': uncertainty['mutual_information']}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aa01ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply UMAP\n",
    "\n",
    "# most important parameter:\n",
    "# number of neighbors employed\n",
    "# for calculating low-dimensional (here, 2D)\n",
    "# embedding\n",
    "n_neighbors_list = [5, 50, 200]\n",
    "# choose Euclidean metric\n",
    "# for measuring distance between data points\n",
    "metric = 'euclidean'\n",
    "# Choose 2 as embedding dimension\n",
    "n_components = 2\n",
    "# plotting parameters\n",
    "s = 2.5\n",
    "edgecolors = 'face'\n",
    "\n",
    "for n_neighbors in n_neighbors_list:\n",
    "    logger.info('Apply UMAP for number of neighbors = {}'.format(n_neighbors))\n",
    "\n",
    "    for key in layer_activations:\n",
    "        \n",
    "        data_for_fitting = layer_activations[key]\n",
    "\n",
    "        mapper1 = umap.UMAP(n_neighbors=n_neighbors, \n",
    "                            metric=metric, \n",
    "                            n_components=n_components).fit(data_for_fitting)\n",
    "        embedding = mapper1.transform(data_for_fitting)\n",
    "\n",
    "        for target in targets[key]:\n",
    "            cmap = None\n",
    "            nber_unique_colors = np.unique(targets[key][target]).size\n",
    "            if target == 'mut_info':\n",
    "                cmap = 'hot'\n",
    "            else:\n",
    "                cmap = 'tab10'\n",
    "            fig, axs = plt.subplots(facecolor='white', figsize=(10, 10))\n",
    "            df = pd.DataFrame({'e1': embedding[:, 0], 'e2': embedding[:, 1], 'target': targets[key][target]})\n",
    "            \n",
    "            if target == 'argmax_pred':\n",
    "                df['target'] = [text_to_numerical_label[_] for _ in df['target'].values]\n",
    "            \n",
    "            im = axs.scatter(df['e1'].values, df['e2'].values, c=df['target'], cmap=cmap, s=s)\n",
    "            axs.set_aspect('equal')\n",
    "            fig.colorbar(im, ax=axs)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # uncomment if want to save\n",
    "            #plt.savefig('{}_{}_nn_{}_embedding.png'.format(key, target, n_neighbors), dpi=200)\n",
    "            #plt.close()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18389007",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_link_fcc = 'https://www.dropbox.com/s/flfy5qe1qxv47t6/Cu_fcc_111.npy?dl=0'\n",
    "\n",
    "!wget -q $download_link_fcc -O 'Cu_fcc_100.npy'\n",
    "\n",
    "image = np.load('Cu_fcc_100.npy')\n",
    "image_name = 'Cu_fcc_100'\n",
    "pixel_to_angstrom = 0.08805239\n",
    "\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a18790",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction, uncertainty = predict_with_uncertainty(data, model, \n",
    "                                                   model_type='classification', \n",
    "                                                   n_iter=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
