{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "428595f4",
   "metadata": {},
   "source": [
    "This notebook provides a step-by-step guide on how to use AI-STEM for analyzing experimental images. \n",
    "\n",
    "The accompanying paper can be found [here](https://doi.org/10.1038/s41524-023-01133-1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85483591",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install 'git+https://github.com/AndreasLeitherer/ai4stem.git'\n",
    "! pip install tensorflow==2.10.0\n",
    "! pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7809904c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import os\n",
    "# tensorflow info/warnings switched off\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import numpy as np\n",
    "import cv2\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 10})\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from ai4stem.utils.utils_data import load_pretrained_model, load_example_image, load_class_dicts\n",
    "from ai4stem.utils.utils_prediction import predict\n",
    "from ai4stem.utils.utils_nn import predict_with_uncertainty\n",
    "from ai4stem.utils.utils_fft import calc_fft\n",
    "from ai4stem.utils.utils_prediction import localwindow\n",
    "\n",
    "numerical_to_text_labels, text_to_numerical_labels = load_class_dicts()\n",
    "\n",
    "# Set logger\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46eea44c",
   "metadata": {},
   "source": [
    "# Quick start\n",
    "\n",
    "After specifying an input image (here, Fe bcc [100] alongside the pixel/Angstrom relation), the following code can be used to analyze it via AI-STEM, employing a pretrained model (which is also used in the [the AI-STEM paper](https://www.nature.com/articles/s41524-023-01133-1))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff60803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "input_image = load_example_image()\n",
    "pixel_to_angstrom = 0.1245 # 1 pixel = 0.1245 Angstrom\n",
    "# Load pretrained model\n",
    "model = load_pretrained_model()\n",
    "# Define window size\n",
    "window_size = int(12. / pixel_to_angstrom)\n",
    "\n",
    "# Analyze image: \n",
    "# return slices of images (fragmentation step, 2D array 'sliced_images'),\n",
    "# calcualted FFT-HAADF descriptors (2D arrray 'fft_descriptors')\n",
    "# neural-network classification propabilities (2D array 'prediction'),\n",
    "# and uncertainty quantification (dictionary 'uncertainty', containing mutual information)\n",
    "sliced_images, fft_descriptors, prediction, uncertainty = predict(input_image, model,\n",
    "                                                                  window_size=window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e41e625",
   "metadata": {},
   "source": [
    "\n",
    "***Note 1***: The neural-network classifier is a *Bayesian* neural network, i.e., its predictions are not deterministic but rather probabilistic - modelling the uncertainty in the predictions, which may arise due to experimental noise  and/or insufficiency of the model parameters. In particular, we employ [Monte Carlo dropout](https://proceedings.mlr.press/v48/gal16.html?trk=public_post_comment-text) to obtain an uncertainty estimate. In practice, several forward passes (here T=100) are calculated to estimate the uncertainty in the predictions (referred to as 'Performing forward pass n/100' in the cell below). In particular, this suffices to identify the expected bulk symmetry and detect the interface as regions of high uncertainty (as quantified by mutual information). We refer to [the AI-STEM paper](https://doi.org/10.48550/arXiv.2303.12702) for more details (in particular the section 'The Bayesian classification model' and Supplementary Figure S4 for more details on how to choose the number of forward passes).\n",
    "\n",
    "***Note 2***: The model is trained on a specific pixel/angstrom relation. Specifically, the model is trained to classify local windows of size 12 Angstrom, where 1 pixel corresponds to 0.12 angstrom. Thus a window size of 12 Angstrom  corresponds to 100 pixels in the simulation settings that we employed for creating the training set. If a different resolution is employed, we recommend to adapt the window size (and this is also done in the above code): given the pixel-to-Angstrom relation, calculate how much pixels correspond to 12 Angstrom and use this as window size. Alternatively, you may rescale the whole image (up/downsampling, e.g., via cv2.resize) such that the resolutions of your input image match the training resolution and then simply use a 100 pixels window size. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd3a391",
   "metadata": {},
   "source": [
    "Now we can visualize the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17acafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(15, 15))\n",
    "# Input image\n",
    "im1 = axs[0].imshow(input_image, cmap='gray')\n",
    "fig.colorbar(im1, ax=axs[0], orientation='vertical', fraction=0.05)\n",
    "axs[0].set_title('Input image')\n",
    "# Assignments: \n",
    "# Get assigned label which corresponds to\n",
    "# most likely class\n",
    "assignments = prediction.argmax(axis=-1)\n",
    "im2 = axs[1].imshow(assignments, cmap='tab10', \n",
    "                    vmin=0, vmax=len(numerical_to_text_labels.keys())-1)\n",
    "axs[1].set_title('Assigned label')\n",
    "# Mutual informatoin\n",
    "im3 = axs[2].imshow(uncertainty, cmap='hot', vmin=0.0)\n",
    "fig.colorbar(im3, ax=axs[2],  orientation='vertical', fraction=0.05)\n",
    "axs[2].set_title('Bayesian uncertainty \\n (mutual information)')\n",
    "\n",
    "# add nice legend for assignments\n",
    "all_colors = plt.cm.tab10.colors\n",
    "unique_assignments = np.unique(assignments.flatten())\n",
    "my_colors = [all_colors[idx] for idx in unique_assignments]\n",
    "patches = [mpatches.Patch(facecolor=c, edgecolor=c) for c in my_colors]\n",
    "axs[1].legend(patches, [numerical_to_text_labels[str(_)] for _ in unique_assignments],\n",
    "              handlelength=0.8, loc='lower right')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd43799",
   "metadata": {},
   "source": [
    "The above calculations provide a quickstart, hiding the individual steps performed in the function 'predict'in from ai4stem.utils.utils_prediction. More detailed explanations are provided in the following."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6acc9fa",
   "metadata": {},
   "source": [
    "# Step-by-step explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6793cb",
   "metadata": {},
   "source": [
    "First we load the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c93d963",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = load_example_image()\n",
    "image_name = 'Fe_bcc'\n",
    "plt.imshow(input_image, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826aebaa",
   "metadata": {},
   "source": [
    "Next, we need the pixel/Anstrom relation. Moreover, the window size and stride employed in the AI-STEM algorithm has to be specified - here the stride is set to a rather coarse value, while more detailed resolution of structural transitions across defects (such as interfaces) may be achieved by decreasing the stride:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33deed8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T09:12:03.793233Z",
     "start_time": "2023-02-15T09:12:03.775703Z"
    }
   },
   "outputs": [],
   "source": [
    "pixel_to_angstrom = 0.1245\n",
    "window_size = 12.\n",
    "stride_size = [36, 36]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f114b1f2",
   "metadata": {},
   "source": [
    "As remarked in the quickstart section, the model is trained on a specific pixel/angstrom relation and window size, so we need to take this into account - where here we do not rescale the image but adapt the window size such that a 12 Angstrom size is obtained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0bfdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapted_window_size = int(window_size * (1. / pixel_to_angstrom))\n",
    "print(adapted_window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd79b07f",
   "metadata": {},
   "source": [
    "Now we can proceed to the first step in the AI-STEM workflow, the fragmentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3650b396",
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_images, _, ni, nj = localwindow(input_image, stride_size=stride_size, \n",
    "                                       pixel_max=adapted_window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd14c20",
   "metadata": {},
   "source": [
    "We may visualize some of the fragments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffe0fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_size = 10\n",
    "fig, axs = plt.subplots(1, 10, figsize=(25, 10))\n",
    "for idx, local_image in enumerate(sliced_images[:selection_size]):\n",
    "    axs[idx].imshow(local_image, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55aeef16",
   "metadata": {},
   "source": [
    "Next, we calculate the FFT-HAADF descriptor for each of the local images, where one particular setting, the application of a threshold to the calculated FFT is discussed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2122b419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFT HAADF descriptor settings\n",
    "# Threshold parameter; given FFT spectrum normalized\n",
    "# to [0, 1], cut off at 0.1 to reduce low-frequency \n",
    "# contributions; default is is to use this setting.\n",
    "thresholding = True # very important\n",
    "\n",
    "fft_descriptors = []\n",
    "for im in sliced_images:\n",
    "    fft_desc = calc_fft(im, thresholding=thresholding)\n",
    "    fft_descriptors.append(fft_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9b3c2d",
   "metadata": {},
   "source": [
    "We may again visualize local fragments and their corresponding 64x64 FFT-HAADF descriptor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d668c9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_size = 10\n",
    "fig, axs = plt.subplots(2, 10, figsize=(25, 5))\n",
    "for idx, local_image in enumerate(sliced_images[:selection_size]):\n",
    "    axs[0, idx].imshow(local_image, cmap='gray')\n",
    "    axs[1, idx].imshow(fft_descriptors[idx], cmap='gray', vmax=0.5) \n",
    "    # vmax=0.5, to enhance visibility of high-freq. peaks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827bb9ff",
   "metadata": {},
   "source": [
    "Next we load the pretrained neural-network model and visualize its architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9f141c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_pretrained_model()\n",
    "model_name = 'pretrained_model'\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db70ad50",
   "metadata": {},
   "source": [
    "The above FFT-HAADF descriptors serve as input to the neural-network classifier. We first adapt the \n",
    "above calculated descriptors such that they fit the model input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc71ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape_from_model = model.layers[0].get_input_at(0).get_shape().as_list()[1:]\n",
    "target_shape = tuple([-1] + input_shape_from_model)\n",
    "nn_input = np.reshape(fft_descriptors, target_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8531f30a",
   "metadata": {},
   "source": [
    "Then, we calculate the neural-network predictions and uncertainty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8110067f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction, uncertainty = predict_with_uncertainty(nn_input, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a01ae0",
   "metadata": {},
   "source": [
    "In the function 'predict_with_uncertainty', other uncertainty quantifiers are calculated as well, while in this work, we explored the use of mutual information.\n",
    "\n",
    "Predictions and uncertainty estimates are not yet in the right shape, they are simply a 1D array and thus we reshape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed50c238",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.reshape(prediction, (ni, nj, prediction.shape[-1]))\n",
    "mutual_information = np.reshape(uncertainty['mutual_information'], (ni, nj))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5aa756",
   "metadata": {},
   "source": [
    "Finally, we can visualize the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0b8aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(15, 15))\n",
    "# Input image\n",
    "im1 = axs[0].imshow(input_image, cmap='gray')\n",
    "fig.colorbar(im1, ax=axs[0], orientation='vertical', fraction=0.05)\n",
    "axs[0].set_title('Input image')\n",
    "# Assignments: \n",
    "# Get assigned label which corresponds to\n",
    "# most likely class\n",
    "assignments = prediction.argmax(axis=-1)\n",
    "im2 = axs[1].imshow(assignments, cmap='tab10',\n",
    "                    vmin=0, vmax=len(numerical_to_text_labels.keys())-1)\n",
    "axs[1].set_title('Assigned label')\n",
    "# Mutual information\n",
    "im3 = axs[2].imshow(mutual_information, cmap='hot', vmin=0.0)\n",
    "fig.colorbar(im3, ax=axs[2],  orientation='vertical', fraction=0.05)\n",
    "axs[2].set_title('Bayesian uncertainty \\n (mutual information)')\n",
    "\n",
    "# add nice legend for assignments\n",
    "all_colors = plt.cm.tab10.colors\n",
    "unique_assignments = np.unique(assignments.flatten())\n",
    "my_colors = [all_colors[idx] for idx in unique_assignments]\n",
    "patches = [mpatches.Patch(facecolor=c, edgecolor=c) for c in my_colors]\n",
    "axs[1].legend(patches, [numerical_to_text_labels[str(_)] for _ in unique_assignments],\n",
    "              handlelength=0.8, loc='lower right')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
