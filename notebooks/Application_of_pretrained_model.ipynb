{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "428595f4",
   "metadata": {},
   "source": [
    "This notebook provides a step-by-step guide on how to use ai4stem for analyzing exerimental images.\n",
    "\n",
    "For a quick start, the following code may be uncommented and executed:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46eea44c",
   "metadata": {},
   "source": [
    "# Quick start\n",
    "\n",
    "After specifying an input image (here, Fe bcc [100]), the following code can be used to analyze it via ai4stem, employing a pretrained model (the one employed in the corresponding publication):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff60803",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai4stem.utils.utils_data import load_pretrained_model, load_example_image\n",
    "from ai4stem.utils.utils_spm import predict\n",
    "\n",
    "input_image = load_example_image()\n",
    "model = load_pretrained_model()\n",
    "\n",
    "sliced_images, fft_descriptors, prediction, uncertainty = predict(input_image, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd43799",
   "metadata": {},
   "source": [
    "Note that the model is trained on a specific pixel/angstrom relation. Specifically, the model is trained to classify local windows of size 12 Angstrom, which corresponds to 100 pixels in the simulation settings that we employed for creating the training set. If a different resolution is employed, we recommend to adapt the window size by , (i.e., pixel/angstrom relation) is employed for the user's application, we recommend to adapt the window size to a We recommend to adapt the window size by  or escale the images\n",
    "\n",
    "More detailed explanations are provided in the following."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6acc9fa",
   "metadata": {},
   "source": [
    "# Step-by-step explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893f030a",
   "metadata": {},
   "source": [
    "First we import some packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223a3d1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T09:12:03.774424Z",
     "start_time": "2023-02-15T09:12:03.770625Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import cv2\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "\n",
    "from ai4stem.utils.utils_data import load_pretrained_model, load_example_image\n",
    "from ai4stem.utils.utils_fft import calc_fft\n",
    "from ai4stem.utils.utils_spm import localwindow\n",
    "from ai4stem.utils.utils_nn import predict_with_uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826aebaa",
   "metadata": {},
   "source": [
    "# Necessary specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33deed8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T09:12:03.793233Z",
     "start_time": "2023-02-15T09:12:03.775703Z"
    }
   },
   "outputs": [],
   "source": [
    "# Specify path where to save the results:\n",
    "results_folder = '.'\n",
    "\n",
    "input_image = load_example_image()\n",
    "image_name = 'Fe_bcc'\n",
    "pixel_to_angstrom = 0.12452489444788318\n",
    "window_size = 12.\n",
    "stride_size = [36, 36]\n",
    "\n",
    "# If want to visualize local windows, set to true\n",
    "save_local_windows = False\n",
    "local_windows_path = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b0ddbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(input_image, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827bb9ff",
   "metadata": {},
   "source": [
    "The following cells do not have to be changed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9f141c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = {image_name: (input_image, \n",
    "                               pixel_to_angstrom)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613cee85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T09:12:04.010767Z",
     "start_time": "2023-02-15T09:12:03.796800Z"
    }
   },
   "outputs": [],
   "source": [
    "# load pretrained model\n",
    "model = load_pretrained_model()\n",
    "model_name = 'pretrained_model'\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c86d9e0",
   "metadata": {},
   "source": [
    "# Analyze image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12961055",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T09:14:59.134088Z",
     "start_time": "2023-02-15T09:12:04.095860Z"
    }
   },
   "outputs": [],
   "source": [
    "# FFT HAADF descriptor settings\n",
    "sigma = None # optional parameter\n",
    "thresholding = True # very important\n",
    "n_iter = 100 # MC dropout samples\n",
    "\n",
    "\n",
    "counter = 0\n",
    "results_dict = defaultdict(dict)\n",
    "for key in filenames:\n",
    "\n",
    "    dx_origin = filenames[key][1]\n",
    "    filename = filenames[key][0]\n",
    "    name = key\n",
    "    \n",
    "    if type(filename) == str:\n",
    "        \n",
    "        img = cv2.imread(filename)\n",
    "        image = deepcopy(img[:, :, 0])\n",
    "    else:\n",
    "        image = filename\n",
    "    adapted_window_size = window_size * (1. / dx_origin)\n",
    "    adapted_window_size = int(round(adapted_window_size))\n",
    "    print('For image called {}, window {} [Angstrom] corresponds to {} pixels'.format(key, window_size, adapted_window_size))\n",
    "\n",
    "    sliced_images, spm_pos, ni, nj = localwindow(image, stride_size=stride_size, pixel_max=adapted_window_size)\n",
    "    np.save(os.path.join(results_folder, '{}_{}_images.npy'.format(name,\n",
    "                                                                       model_name)), sliced_images)\n",
    "    \n",
    "    fft_descriptors = []\n",
    "    for im in sliced_images:\n",
    "        fft_desc = calc_fft(im, sigma=sigma, thresholding=thresholding)\n",
    "        fft_descriptors.append(fft_desc)\n",
    "    np.save(os.path.join(results_folder, '{}_fft_desc.npy'.format(name)), np.asarray(fft_descriptors))\n",
    "\n",
    "    repeated_images = np.array([np.stack([_]) for _ in fft_descriptors])\n",
    "    repeated_images = np.moveaxis(repeated_images, 1, -1)\n",
    "\n",
    "\n",
    "    prediction, uncertainty = predict_with_uncertainty(repeated_images, \n",
    "                                                   model=model, \n",
    "                                                   model_type='classification', \n",
    "                                                   n_iter=n_iter)\n",
    "    np.save(os.path.join(results_folder, '{}_{}_predictions.npy'.format(name,\n",
    "                                                                       model_name)), prediction)\n",
    "    for key in uncertainty:\n",
    "        np.save(os.path.join(results_folder, '{}_{}_{}.npy'.format(name,\n",
    "                                                                  model_name,\n",
    "                                                                  key)), uncertainty[key])\n",
    "    argmax_pred = prediction.argmax(axis=-1)\n",
    "    argmax_pred = np.reshape(argmax_pred, (ni, nj))\n",
    "    mutinfo = uncertainty['mutual_information']\n",
    "    mutinfo = np.reshape(mutinfo, (ni, nj))\n",
    "\n",
    "    results_dict[model_name][name] = {}\n",
    "    results_dict[model_name][name]['Prediction'] = argmax_pred\n",
    "    results_dict[model_name][name]['Mutual information'] = mutinfo\n",
    "    results_dict[model_name][name]['Input Image'] = image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e15414",
   "metadata": {},
   "source": [
    "# Visualize predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ff4e2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T09:14:59.956188Z",
     "start_time": "2023-02-15T09:14:59.135324Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "matplotlib.rcParams.update({'font.size': 10})\n",
    "\n",
    "\n",
    "for key in filenames:\n",
    "    name = key\n",
    "\n",
    "    argmax_pred = results_dict[model_name][name]['Prediction']\n",
    "    mutinfo = results_dict[model_name][name]['Mutual information']\n",
    "    image = results_dict[model_name][name]['Input Image']\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(10, 10))\n",
    "\n",
    "\n",
    "    im1 = axs[0].imshow(image, cmap='gray')\n",
    "    fig.colorbar(im1, ax=axs[0], orientation='vertical')\n",
    "\n",
    "    im2 = axs[1].imshow(argmax_pred, cmap='tab20')\n",
    "    fig.colorbar(im2, ax=axs[1],  orientation='vertical')\n",
    "\n",
    "    im3 = axs[2].imshow(mutinfo, cmap='hot', vmin=0.0)\n",
    "    fig.colorbar(im3, ax=axs[2],  orientation='vertical')\n",
    "    \n",
    "    axs[0].axis('off')\n",
    "    axs[1].axis('off')\n",
    "    axs[2].axis('off')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6272e851",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
