{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "428595f4",
   "metadata": {},
   "source": [
    "This notebook provides a step-by-step guide on how to use ai4stem for analyzing exerimental images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85483591",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install 'git+https://github.com/AndreasLeitherer/ai4stem.git'\n",
    "! pip install tensorflow\n",
    "! pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46eea44c",
   "metadata": {},
   "source": [
    "# Quick start\n",
    "\n",
    "After specifying an input image (here, Fe bcc [100]), the following code can be used to analyze it via ai4stem, employing a pretrained model (which is also employed in the ai4stem publication):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff60803",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# tensorflow info/warnings switched off\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "from ai4stem.utils.utils_data import load_pretrained_model, load_example_image\n",
    "from ai4stem.utils.utils_prediction import predict\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "input_image = load_example_image()\n",
    "model = load_pretrained_model()\n",
    "\n",
    "sliced_images, fft_descriptors, prediction, uncertainty = predict(input_image, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd43799",
   "metadata": {},
   "source": [
    "*Note:*\n",
    "\n",
    "The model is trained on a specific pixel/angstrom relation. Specifically, the model is trained to classify local windows of size 12 Angstrom, which corresponds to 100 pixels in the simulation settings that we employed for creating the training set. If a different resolution is employed, we recommend to adapt the window size: given the pixel-to-Angstrom relation, calculate how much pixels correspond to 12 Angstrom and use this as window size. This is also exemplarily performed in the more detailed explanations further below. Alternatively, you may rescale the whole image (up/downsampling) such that the resolutions of your input image match the training resolution and then simply use a 100 pixels window size.\n",
    "\n",
    "After this quickstart, more detailed explanations are provided in the following."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6acc9fa",
   "metadata": {},
   "source": [
    "# Step-by-step explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893f030a",
   "metadata": {},
   "source": [
    "First we import some packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223a3d1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T09:12:03.774424Z",
     "start_time": "2023-02-15T09:12:03.770625Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import cv2\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "\n",
    "from ai4stem.utils.utils_data import load_pretrained_model, load_example_image\n",
    "from ai4stem.utils.utils_fft import calc_fft\n",
    "from ai4stem.utils.utils_prediction import localwindow\n",
    "from ai4stem.utils.utils_nn import predict_with_uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826aebaa",
   "metadata": {},
   "source": [
    "# Necessary specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33deed8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T09:12:03.793233Z",
     "start_time": "2023-02-15T09:12:03.775703Z"
    }
   },
   "outputs": [],
   "source": [
    "# Specify path where to save the results:\n",
    "results_folder = '.'\n",
    "\n",
    "input_image = load_example_image()\n",
    "image_name = 'Fe_bcc'\n",
    "pixel_to_angstrom = 0.12452489444788318\n",
    "window_size = 12.\n",
    "stride_size = [36, 36]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b0ddbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(input_image, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827bb9ff",
   "metadata": {},
   "source": [
    "The following cells do not have to be changed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9f141c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = {image_name: (input_image, \n",
    "                               pixel_to_angstrom)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613cee85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T09:12:04.010767Z",
     "start_time": "2023-02-15T09:12:03.796800Z"
    }
   },
   "outputs": [],
   "source": [
    "# load pretrained model\n",
    "model = load_pretrained_model()\n",
    "model_name = 'pretrained_model'\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c86d9e0",
   "metadata": {},
   "source": [
    "# Analyze image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12961055",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T09:14:59.134088Z",
     "start_time": "2023-02-15T09:12:04.095860Z"
    }
   },
   "outputs": [],
   "source": [
    "# FFT HAADF descriptor settings\n",
    "# optional parameter for cutting low-frequency part\n",
    "# by applying gaussian mask of width sigma; default setting\n",
    "# is to not use this.\n",
    "sigma = None\n",
    "# Thresholding parameter; given FFT spectrum normalized\n",
    "# to [0, 1], cut off at 0.1 to reduce low-frequency \n",
    "# contributions; default is is to use this setting.\n",
    "thresholding = True # very important\n",
    "n_iter = 100 # MC dropout samples\n",
    "\n",
    "\n",
    "counter = 0\n",
    "results_dict = defaultdict(dict)\n",
    "for key in filenames:\n",
    "    logger.info('Convert window size in Angstrom to window size in pixels.')\n",
    "    dx_origin = filenames[key][1]\n",
    "    filename = filenames[key][0]\n",
    "    name = key\n",
    "    \n",
    "    if type(filename) == str:\n",
    "        \n",
    "        img = cv2.imread(filename)\n",
    "        image = deepcopy(img[:, :, 0])\n",
    "    else:\n",
    "        image = filename\n",
    "    adapted_window_size = window_size * (1. / dx_origin)\n",
    "    adapted_window_size = int(round(adapted_window_size))\n",
    "    logger.info('For image called {}, window {} [Angstrom] corresponds to {} pixels'.format(key, \n",
    "                                                                                            window_size, \n",
    "                                                                                            adapted_window_size))\n",
    "\n",
    "    logger.info('Perform fragmentation.')\n",
    "    sliced_images, spm_pos, ni, nj = localwindow(image, stride_size=stride_size, pixel_max=adapted_window_size)\n",
    "    np.save(os.path.join(results_folder, '{}_{}_images.npy'.format(name,\n",
    "                                                                       model_name)), sliced_images)\n",
    "    logger.info('Calculate FFT-HAADF descriptor.')\n",
    "    fft_descriptors = []\n",
    "    for im in sliced_images:\n",
    "        fft_desc = calc_fft(im, sigma=sigma, thresholding=thresholding)\n",
    "        fft_descriptors.append(fft_desc)\n",
    "    np.save(os.path.join(results_folder, '{}_fft_desc.npy'.format(name)), np.asarray(fft_descriptors))\n",
    "\n",
    "    repeated_images = np.array([np.stack([_]) for _ in fft_descriptors])\n",
    "    repeated_images = np.moveaxis(repeated_images, 1, -1)\n",
    "\n",
    "    logger.info('Calculate neural-network predictions and uncertainty.')\n",
    "    prediction, uncertainty = predict_with_uncertainty(repeated_images, \n",
    "                                                   model=model, \n",
    "                                                   model_type='classification', \n",
    "                                                   n_iter=n_iter)\n",
    "    np.save(os.path.join(results_folder, '{}_{}_predictions.npy'.format(name,\n",
    "                                                                       model_name)), prediction)\n",
    "    for key in uncertainty:\n",
    "        np.save(os.path.join(results_folder, '{}_{}_{}.npy'.format(name,\n",
    "                                                                  model_name,\n",
    "                                                                  key)), uncertainty[key])\n",
    "    argmax_pred = prediction.argmax(axis=-1)\n",
    "    argmax_pred = np.reshape(argmax_pred, (ni, nj))\n",
    "    mutinfo = uncertainty['mutual_information']\n",
    "    mutinfo = np.reshape(mutinfo, (ni, nj))\n",
    "\n",
    "    results_dict[model_name][name] = {}\n",
    "    results_dict[model_name][name]['Prediction'] = argmax_pred\n",
    "    results_dict[model_name][name]['Mutual information'] = mutinfo\n",
    "    results_dict[model_name][name]['Input Image'] = image\n",
    "    logger.info('Calculation for image {} finished.'.format(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e15414",
   "metadata": {},
   "source": [
    "# Visualize predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ff4e2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T09:14:59.956188Z",
     "start_time": "2023-02-15T09:14:59.135324Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "matplotlib.rcParams.update({'font.size': 10})\n",
    "\n",
    "\n",
    "for key in filenames:\n",
    "    name = key\n",
    "\n",
    "    argmax_pred = results_dict[model_name][name]['Prediction']\n",
    "    mutinfo = results_dict[model_name][name]['Mutual information']\n",
    "    image = results_dict[model_name][name]['Input Image']\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(10, 10))\n",
    "\n",
    "\n",
    "    im1 = axs[0].imshow(image, cmap='gray')\n",
    "    fig.colorbar(im1, ax=axs[0], orientation='vertical', fraction=0.05)\n",
    "    axs[0].set_title('Input image')\n",
    "\n",
    "    im2 = axs[1].imshow(argmax_pred, cmap='tab10')\n",
    "    fig.colorbar(im2, ax=axs[1],  orientation='vertical', fraction=0.05)\n",
    "    axs[1].set_title('Assigned label')\n",
    "\n",
    "    im3 = axs[2].imshow(mutinfo, cmap='hot', vmin=0.0)\n",
    "    fig.colorbar(im3, ax=axs[2],  orientation='vertical', fraction=0.05)\n",
    "    axs[2].set_title('Bayesian uncertainty \\n (mutual information)')\n",
    "    \n",
    "    axs[0].axis('off')\n",
    "    axs[1].axis('off')\n",
    "    axs[2].axis('off')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6272e851",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
